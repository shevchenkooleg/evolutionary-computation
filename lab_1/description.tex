\documentclass[12pt]{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{caption}
\geometry{a4paper, left=20mm, right=15mm, top=20mm, bottom=20mm}


\title{Лабораторная работа №1 \\ Исследование градиентного спуска для квадратичной формы}
\author{Студент: Шевченко О.В. \\ Группа: 09.04.01-ПОВа-з25}
\date{\today}

\begin{document}

\maketitle

\section{Цель работы}
\label{sec:goal}

Данная лабораторная работа посвящена практическому изучению алгоритма градиентного спуска для минимизации функции многих переменных. В качестве объекта исследования выбрана квадратичная форма вида:
\[
f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^\top A \mathbf{x},
\]
где $\mathbf{x} \in \mathbb{R}^n$, а $A$ -- симметричная положительно определённая матрица ($A = A^\top \succ 0$).

Конкретные цели работы включают:

\begin{enumerate}
    \item \textbf{Практическая реализация:} Освоение процедуры программирования алгоритма градиентного спуска для гладкой выпуклой функции.
    \item \textbf{Исследование параметров:} Систематическое изучение влияния размера шага (скорости обучения $\alpha$) на скорость сходимости и устойчивость метода.
    \item \textbf{Визуальный анализ:} Для случая $n=2$ -- построение и анализ траекторий спуска в пространстве параметров на фоне линий уровня целевой функции.
    \item \textbf{Сравнение с теорией:} Проверка на практике теоретических оценок скорости сходимости и условий устойчивости, связанных со спектральными свойствами матрицы $A$ (собственными значениями $\lambda_{\max}$, $\lambda_{\min}$ и числом обусловленности $\kappa = \frac{\lambda_{\max}}{\lambda_{\min}}$).
\end{enumerate}

Результатом работы должно стать не только корректно работающее программное обеспечение, но и качественное понимание ключевых факторов, определяющих эффективность одного из фундаментальных методов непрерывной оптимизации.

\section{Теоретическая часть}
\label{sec:theory}

\subsection{Постановка задачи оптимизации}
\label{subsec:problem}

Требуется найти точку глобального минимума $\mathbf{x}^* \in \mathbb{R}^n$ функции $f(\mathbf{x})$ и соответствующее минимальное значение $f^*$:
\[
\mathbf{x}^* = \arg\min_{\mathbf{x} \in \mathbb{R}^n} f(\mathbf{x}), \quad f^* = f(\mathbf{x}^*).
\]
В силу свойств матрицы $A$ (симметричность и положительная определённость) задача имеет единственное решение.

\subsection{Свойства целевой функции}
\label{subsec:properties}

\begin{enumerate}
    \item \textbf{Градиент} функции вычисляется по формуле:
    \[
    \nabla f(\mathbf{x}) = A \mathbf{x}.
    \]
    Необходимое и достаточное условие минимума $\nabla f(\mathbf{x}^*) = 0$ приводит к системе линейных уравнений $A \mathbf{x}^* = 0$. Так как $A$ положительно определена (невырождена), решение единственно: $\mathbf{x}^* = \mathbf{0}$. Следовательно, $f^* = 0$.

    \item \textbf{Матрица Гессе} (гессиан) функции постоянна и равна $A$:
    \[
    \nabla^2 f(\mathbf{x}) = A.
    \]
    Положительная определённость $A$ гарантирует строгую выпуклость функции $f(\mathbf{x})$, что обеспечивает сходимость градиентного метода к глобальному минимуму при правильном выборе шага.

    \item \textbf{Число обусловленности} задачи определяется собственными значениями матрицы $A$. Пусть $0 < \lambda_{\min} = \lambda_1 \leq \lambda_2 \leq \dots \leq \lambda_n = \lambda_{\max}$. Тогда число обусловленности $\kappa$ есть:
    \[
    \kappa(A) = \frac{\lambda_{\max}}{\lambda_{\min}} \geq 1.
    \]
    Величина $\kappa$ напрямую влияет на геометрию линий уровня функции $f(\mathbf{x})$. При $\kappa \approx 1$ (хорошая обусловленность) линии уровня близки к сферам. При $\kappa \gg 1$ (плохая обусловленность) они сильно вытянуты (эллипсоиды), что замедляет сходимость градиентного спуска.
\end{enumerate}

\subsection{Алгоритм градиентного спуска}
\label{subsec:algorithm}

Основная итерационная формула градиентного спуска с постоянным шагом $\alpha > 0$ имеет вид:
\begin{equation}
\label{eq:gd_iter}
\mathbf{x}_{k+1} = \mathbf{x}_k - \alpha \nabla f(\mathbf{x}_k) = \mathbf{x}_k - \alpha A \mathbf{x}_k = (I - \alpha A) \mathbf{x}_k,
\end{equation}
где $k = 0, 1, 2, \dots$ -- номер итерации, $I$ -- единичная матрица.

\subsection{Условия сходимости и выбор шага}
\label{subsec:convergence}

Скорость сходимости итерационного процесса (\ref{eq:gd_iter}) определяется спектральным радиусом матрицы перехода $G = I - \alpha A$. Для сходимости метода ( $\|\mathbf{x}_k\| \to 0$ ) необходимо и достаточно, чтобы $\rho(G) < 1$, где $\rho(G) = \max_i |1 - \alpha \lambda_i|$.

\begin{itemize}
    \item \textbf{Условие устойчивости:} Сходимость гарантирована при
    \begin{equation}
    \label{eq:alpha_range}
    0 < \alpha < \frac{2}{\lambda_{\max}}.
    \end{equation}

    \item \textbf{Оптимальный постоянный шаг} (минимизирующий наихудшую оценку скорости сходимости) задаётся выражением:
    \begin{equation}
    \label{eq:alpha_opt}
    \alpha_{\text{опт}} = \frac{2}{\lambda_{\min} + \lambda_{\max}}.
    \end{equation}
    При этом значении $\alpha$ гарантированная скорость сходимости по функции является линейной:
    \[
    f(\mathbf{x}_k) \leq C \cdot q^k, \quad \text{где } q = \frac{\kappa - 1}{\kappa + 1} < 1.
    \]
    Чем больше $\kappa$, тем ближе $q$ к 1 и медленнее сходимость.

    \item \textbf{Критерии остановки:} На практике итерации прекращают при выполнении одного из условий:
    \begin{align}
    \|\nabla f(\mathbf{x}_k)\| &< \varepsilon_1 \quad \text{(малость градиента)}, \label{eq:stop1}\\
    \|\mathbf{x}_{k+1} - \mathbf{x}_k\| &< \varepsilon_2 \quad \text{(малость приращения аргумента)}, \label{eq:stop2}\\
    |f(\mathbf{x}_{k+1}) - f(\mathbf{x}_k)| &< \varepsilon_3 \quad \text{(малость приращения функции)}, \label{eq:stop3}\\
    k &> K_{\max} \quad \text{(превышено максимальное число итераций)}. \label{eq:stop4}
    \end{align}
    Типичные значения: $\varepsilon_1, \varepsilon_2, \varepsilon_3 \in [10^{-6}, 10^{-8}]$, $K_{\max} \sim 10^3 - 10^5$.
\end{itemize}

\maketitle

\section{Практическая часть}
\label{sec:practical}

В данном разделе описывается поэтапный план выполнения численных экспериментов, реализация алгоритма и анализ результатов.

\subsection{Этап 1: Подготовка данных и среды}
\label{subsec:setup}

\subsubsection{Выбор параметров задачи}
\begin{enumerate}
    \item \textbf{Размерность пространства:} Для наглядной визуализации траекторий выбрана размерность $n = 2$. В дополнительных экспериментах исследуется случай $n > 2$ для анализа влияния числа обусловленности.

    \item \textbf{Матрица $A$:} Зададим симметричную положительно определенную матрицу:
    \[
    A = \begin{pmatrix}
    3 & 1 \\
    1 & 2
    \end{pmatrix}.
    \]
    \begin{itemize}
        \item Собственные значения: $\lambda_1 \approx 1.382$, $\lambda_2 \approx 3.618$
        \item Число обусловленности: $\kappa = \frac{\lambda_{\max}}{\lambda_{\min}} \approx \frac{3.618}{1.382} \approx 2.618$
        \item Теоретический диапазон шага: $0 < \alpha < \frac{2}{\lambda_{\max}} \approx 0.553$
        \item Оптимальный шаг: $\alpha_{\text{опт}} = \frac{2}{\lambda_{\min} + \lambda_{\max}} = \frac{2}{1.382 + 3.618} = 0.4$
    \end{itemize}

    \item \textbf{Начальная точка:} $x_0 = (10, 10)^\top$

    \item \textbf{Критерии остановки:}
    \begin{align*}
        \text{Максимальное число итераций:} & \quad N_{\max} = 1000 \\
        \text{Точность по градиенту:} & \quad \|\nabla f(x_k)\| < \varepsilon_1 = 10^{-6} \\
        \text{Точность по функции:} & \quad |f(x_{k+1}) - f(x_k)| < \varepsilon_2 = 10^{-8}
    \end{align*}
\end{enumerate}

\subsubsection{Инструменты реализации}
Для реализации алгоритма и визуализации результатов используется язык Python 3.x с библиотеками:
\begin{itemize}
    \item \texttt{numpy} - для матричных вычислений
    \item \texttt{matplotlib} - для построения графиков
    \item \texttt{scipy.linalg} - для вычисления собственных значений
\end{itemize}

\subsection{Этап 2: Реализация базового алгоритма}
\label{subsec:implementation}

\subsubsection{Псевдокод алгоритма}
\begin{verbatim}
Вход: A, x0, alpha, max_iter, tol_grad, tol_func
Выход: x_opt, f_opt, history

1. Инициализация:
   x = x0
   history = []
   k = 0

2. Пока (k < max_iter):
   а) Вычислить градиент: grad = A @ x
   б) Вычислить значение функции: f_val = 0.5 * x.T @ A @ x
   в) Сохранить в history: (x, f_val, norm(grad))
   г) Проверить критерии остановки:
      если norm(grad) < tol_grad или
         |f_val - history[-2].f_val| < tol_func:
         прервать цикл
   д) Обновить точку: x = x - alpha * grad
   е) k = k + 1

3. Вернуть x, f_val, history
\end{verbatim}

\subsubsection{Функция градиентного спуска на Python}
\begin{verbatim}
import numpy as np

def gradient_descent(A, x0, alpha=0.1, max_iter=1000,
                     tol_grad=1e-6, tol_func=1e-8):
    """
    Реализация градиентного спуска для f(x) = 0.5 * x^T A x

    Parameters:
    A : numpy.ndarray - симметричная положительно определенная матрица
    x0 : numpy.ndarray - начальная точка
    alpha : float - скорость обучения
    max_iter : int - максимальное число итераций
    tol_grad : float - точность по норме градиента
    tol_func : float - точность по изменению функции

    Returns:
    x_opt : numpy.ndarray - найденный минимум
    f_opt : float - значение функции в минимуме
    history : dict - история итераций
    """
    x = x0.copy()
    n = len(x0)

    # Инициализация истории
    history = {
        'x': [x.copy()],
        'f': [],
        'grad_norm': []
    }

    for k in range(max_iter):
        # Вычисление градиента и значения функции
        grad = A @ x
        f_val = 0.5 * x.T @ A @ x

        # Сохранение в историю
        history['f'].append(f_val)
        history['grad_norm'].append(np.linalg.norm(grad))

        # Проверка критериев остановки
        if k > 0:
            if (np.linalg.norm(grad) < tol_grad or
                abs(history['f'][-1] - history['f'][-2]) < tol_func):
                break

        # Обновление точки
        x = x - alpha * grad
        history['x'].append(x.copy())

    # Последнее значение функции
    f_opt = 0.5 * x.T @ A @ x
    history['x'] = history['x'][:-1]  # Убираем лишний x

    return x, f_opt, history
\end{verbatim}

\subsection{Этап 3: Эксперименты и анализ}
\label{subsec:experiments}

\subsubsection{Задание 3.1: Влияние скорости обучения $\alpha$}
\label{subsubsec:alpha_influence}

\begin{enumerate}
    \item \textbf{Теоретические границы:} Для матрицы $A = \begin{pmatrix} 3 & 1 \\ 1 & 2 \end{pmatrix}$:
    \begin{itemize}
        \item $\lambda_{\max} \approx 3.618$, $\lambda_{\min} \approx 1.382$
        \item Диапазон сходимости: $0 < \alpha < \dfrac{2}{\lambda_{\max}} \approx 0.553$
        \item Оптимальный шаг: $\alpha_{\text{опт}} = \dfrac{2}{\lambda_{\min} + \lambda_{\max}} = 0.4$
    \end{itemize}

    \item \textbf{Выбранные значения $\alpha$ для эксперимента:}
    \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Случай} & \textbf{Значение $\alpha$} & \textbf{Отношение к $\alpha_{\max}$} \\ \hline
    Очень маленький & 0.05 & $\approx 0.09\alpha_{\max}$ \\ \hline
    Маленький & 0.1 & $\approx 0.18\alpha_{\max}$ \\ \hline
    Оптимальный & 0.4 & $\approx 0.72\alpha_{\max}$ \\ \hline
    Близкий к пределу & 0.52 & $\approx 0.94\alpha_{\max}$ \\ \hline
    Сверх предельного & 0.6 & $> \alpha_{\max}$ \\ \hline
    \end{tabular}
    \caption{Значения шага $\alpha$ для исследования}
    \label{tab:alpha_values}
    \end{table}

    \item \textbf{Методика эксперимента:} Для каждого $\alpha$ из таблицы \ref{tab:alpha_values}:
    \begin{itemize}
        \item Запустить алгоритм градиентного спуска с начальной точкой $x_0 = (10, 10)^\top$
        \item Критерии остановки: $\|\nabla f(x_k)\| < 10^{-6}$ или 1000 итераций
        \item Записать количество итераций до сходимости
        \item Записать финальную норму градиента и значение функции
        \item Сохранить историю значений функции и нормы градиента
    \end{itemize}

    \item \textbf{Полученные результаты:}
    \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    $\alpha$ & \textbf{Итерации} & $\mathbf{f(x^*)}$ & $\mathbf{\|\nabla f(x^*)\|}$ & \textbf{Сходимость} \\ \hline
    0.05 & 130 & $5.998\times10^{-8}$ & $4.374\times10^{-4}$ & ✓ \\ \hline
    0.1 & 66 & $2.172\times10^{-8}$ & $2.843\times10^{-4}$ & ✓ \\ \hline
    0.4 & 16 & $2.294\times10^{-9}$ & $2.862\times10^{-4}$ & ✓ \\ \hline
    0.52 & 92 & $2.784\times10^{-8}$ & $5.093\times10^{-4}$ & ✓ \\ \hline
    0.6 & 1000 & $3.277\times10^{139}$ & $1.315\times10^{70}$ & ✗ \\ \hline
    \end{tabular}
    \caption{Экспериментальные результаты влияния $\alpha$}
    \label{tab:alpha_results}
    \end{table}

    \item \textbf{Анализ результатов:}
    \begin{enumerate}
        \item \textbf{Оптимальный шаг ($\alpha = 0.4$)} показал наилучшую сходимость:
        \begin{itemize}
            \item Минимальное число итераций (16)
            \item Наименьшее значение функции в точке остановки
            \item Подтверждение теоретического расчёта $\alpha_{\text{опт}} = 0.4$
        \end{itemize}

        \item \textbf{Малые шаги ($\alpha = 0.05, 0.1$)} обеспечили сходимость, но медленно:
        \begin{itemize}
            \item $\alpha = 0.05$: 130 итераций (в 8.1 раза больше оптимального)
            \item $\alpha = 0.1$: 66 итераций (в 4.1 раза больше оптимального)
            \item Консервативные шаги гарантируют сходимость, но неэффективны
        \end{itemize}

        \item \textbf{Близкий к пределу шаг ($\alpha = 0.52$)}:
        \begin{itemize}
            \item 92 итерации (в 5.8 раза больше оптимального)
            \item Колебательное поведение вокруг минимума
            \item Норма градиента выше, чем при оптимальном $\alpha$
        \end{itemize}

        \item \textbf{Сверх предельного шага ($\alpha = 0.6$)}:
        \begin{itemize}
            \item Алгоритм не сошёлся за 1000 итераций
            \item Экспоненциальный рост значений функции и градиента
            \item Подтверждение теоретического условия $\alpha < \alpha_{\max}$
        \end{itemize}
    \end{enumerate}

    \item \textbf{Визуализация результатов:}
    \begin{enumerate}
        \item Построен график $f(x_k)$ от номера итерации $k$ в логарифмическом масштабе (Рисунок \ref{fig:convergence_alpha})
        \item Построен график $\|\nabla f(x_k)\|$ от номера итерации $k$ (Рисунок \ref{fig:gradient_alpha})
        \item Построены траектории спуска на линиях уровня функции (Рисунок \ref{fig:trajectories_alpha})
    \end{enumerate}
\end{enumerate}

\subsubsection{Задание 3.2: Визуализация траекторий (для $n=2$)}
\label{subsubsec:trajectories}

\begin{enumerate}
    \item \textbf{Методика визуализации:}
    \begin{itemize}
        \item Построение линий уровня функции $f(x) = \frac{1}{2}x^T A x$
        \item Отображение траекторий для разных значений $\alpha$
        \item Отметка начальной и конечной точек
    \end{itemize}

    \item \textbf{Наблюдаемые траектории:}
    \begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|c|p{6cm}|p{10cm}|}
\hline
\textbf{$\alpha$} &
\textbf{Характер траектории} &
\textbf{Объяснение} \\ \hline

0.05 &
Медленная, плавная, \newline
много мелких шагов &
Маленькие шаги обеспечивают точность, \newline
но приводят к медленной сходимости \\ \hline

0.4 &
Прямая или почти прямая \newline
к минимуму &
Оптимальный баланс между размером шага \newline
и направлением движения \\ \hline

0.52 &
Зигзагообразная, \newline
с перескакиванием &
Слишком большие шаги приводят \newline
к колебаниям вокруг минимума \\ \hline

0.6 &
Расходящаяся \newline
спираль &
Превышение критического \newline
значения параметра $\alpha$ \\ \hline

\end{tabular}
\caption{Характеристики траекторий для разных значений $\alpha$}
\label{tab:trajectory_analysis}
\end{table}

    \item \textbf{Анализ геометрии:}
    \begin{itemize}
        \item \textbf{Собственные векторы матрицы $A$}:
        \[
        v_1 = \begin{pmatrix} 0.5257 \\ -0.8507 \end{pmatrix} \quad (\lambda_{\min} = 1.382)
        \]
        \[
        v_2 = \begin{pmatrix} -0.8507 \\ -0.5257 \end{pmatrix} \quad (\lambda_{\max} = 3.618)
        \]

        \item \textbf{Направление $v_1$}: наиболее пологий спуск (малая кривизна)
        \item \textbf{Направление $v_2$}: наиболее крутой спуск (большая кривизна)
        \item Зигзагообразные траектории возникают из-за разной скорости движения вдоль этих направлений
    \end{itemize}

    \item \textbf{Ключевые наблюдения:}
    \begin{enumerate}
        \item При оптимальном $\alpha$ градиентный спуск следует собственному вектору, соответствующему наибольшему собственному значению
        \item При плохом выборе $\alpha$ возникают колебания между направлениями разных собственных векторов
        \item Направление градиента всегда перпендикулярно линии уровня в текущей точке
    \end{enumerate}
\end{enumerate}

\subsubsection{Задание 3.3: Влияние числа обусловленности $\kappa$}
\label{subsubsec:condition_number}

\begin{enumerate}
    \item \textbf{Матрицы для исследования:}
    \begin{itemize}
        \item Хорошо обусловленная: $\kappa \approx 2.6$ ($A = \begin{pmatrix} 3 & 1 \\ 1 & 2 \end{pmatrix}$)
        \item Плохо обусловленная: $\kappa \approx 10$ ($A = \begin{pmatrix} 10 & 0 \\ 0 & 1 \end{pmatrix}$)
    \end{itemize}

    \item \textbf{Параметры эксперимента:}
    \begin{itemize}
        \item Начальная точка: $x_0 = (5, 5)^\top$
        \item Шаг: $\alpha = \alpha_{\text{опт}}$ для каждой матрицы
        \item Критерии остановки: $\|\nabla f(x_k)\| < 10^{-6}$
    \end{itemize}

    \item \textbf{Полученные результаты:}
    \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Матрица} & \textbf{$\kappa$} & \textbf{Итерации} & \textbf{Скорость (1/итерации)} \\ \hline
    Хорошо обусловленная & 2.6 & 16 & 0.0625 \\ \hline
    Плохо обусловленная & 10.0 & 57 & 0.0175 \\ \hline
    \end{tabular}
    \caption{Влияние числа обусловленности на сходимость}
    \label{tab:kappa_results}
    \end{table}

    \item \textbf{Анализ результатов:}
    \begin{enumerate}
        \item \textbf{Количественное сравнение}:
        \begin{itemize}
            \item При $\kappa = 2.6$: сходимость за 16 итераций
            \item При $\kappa = 10.0$: сходимость за 57 итераций
            \item Увеличение итераций в 3.6 раза при увеличении $\kappa$ в 3.8 раза
        \end{itemize}

        \item \textbf{Теоретическое обоснование}:
        \[
        \frac{f(x_k)}{f(x_0)} \leq \left(\frac{\kappa - 1}{\kappa + 1}\right)^{2k}
        \]
        \begin{itemize}
            \item Для $\kappa = 2.6$: коэффициент сходимости $q \approx 0.447$
            \item Для $\kappa = 10$: коэффициент сходимости $q \approx 0.818$
            \item Большее $q$ означает более медленную сходимость
        \end{itemize}

        \item \textbf{Причины замедления сходимости}:
        \begin{enumerate}
            \item Сильно вытянутые линии уровня (эллипсы с большим эксцентриситетом)
            \item Градиент указывает почти перпендикулярно направлению к минимуму
            \item Много зигзагообразных движений для коррекции направления
        \end{enumerate}
    \end{enumerate}

    \item \textbf{Графический анализ:}
    \begin{itemize}
        \item Для плохо обусловленной матрицы линии уровня сильно вытянуты
        \item Траектория градиентного спуска имеет выраженный зигзагообразный характер
        \item Даже при оптимальном $\alpha$ сходимость значительно медленнее
    \end{itemize}
\end{enumerate}

\subsubsection{Анализ результатов экспериментов}
\label{subsubsec:results_analysis}

\begin{enumerate}
    \item \textbf{Сводная таблица результатов:}
    \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    \textbf{Эксперимент} & \textbf{Итерации} & $\mathbf{\|\nabla f\|}$ & $\mathbf{f(x^*)}$ & \textbf{Время (с)} \\ \hline
    $\alpha=0.05$ & 130 & $4.374\times10^{-4}$ & $5.998\times10^{-8}$ & 0.001183 \\ \hline
    $\alpha=0.4$ & 16 & $2.862\times10^{-4}$ & $2.294\times10^{-9}$ & 0.000181 \\ \hline
    $\alpha=0.52$ & 92 & $5.093\times10^{-4}$ & $2.784\times10^{-8}$ & 0.000634 \\ \hline
    $\kappa=2.6$ & 16 & $2.862\times10^{-4}$ & $2.294\times10^{-9}$ & 0.000181 \\ \hline
    $\kappa=10.0$ & 57 & $3.215\times10^{-4}$ & $4.127\times10^{-9}$ & 0.000521 \\ \hline
    \end{tabular}
    \caption{Сводные результаты всех экспериментов}
    \label{tab:summary_results}
    \end{table}

    \item \textbf{Качественный анализ:}
    \begin{enumerate}
        \item \textbf{Сравнение теоретических и практических границ $\alpha$}:
        \begin{itemize}
            \item Теоретический диапазон: $0 < \alpha < 0.553$
            \item Экспериментальное подтверждение: $\alpha = 0.6$ приводит к расходимости
            \item Практический оптимальный шаг $\alpha = 0.4$ совпадает с теоретическим
        \end{itemize}

        \item \textbf{Объяснение формы траекторий через собственные векторы $A$}:
        \begin{itemize}
            \item Направления собственных векторов определяют ориентацию эллипсов уровня
            \item Разная скорость движения вдоль разных собственных направлений
            \item Зигзаги возникают из-за попеременного движения вдоль разных осей
        \end{itemize}

        \item \textbf{Анализ причин замедления сходимости при плохой обусловленности}:
        \begin{enumerate}
            \item Геометрическая причина: вытянутые линии уровня
            \item Алгоритмическая причина: неэффективное использование информации о градиенте
            \item Математическая причина: большая разница между $\lambda_{\min}$ и $\lambda_{\max}$
        \end{enumerate}

        \item \textbf{Рекомендации по выбору $\alpha$ для реальных задач}:
        \begin{enumerate}
            \item Начинать с консервативного шага $\alpha \approx 0.1 \times \alpha_{\max}$
            \item Использовать адаптивные методы (линейный поиск, методы с моментом)
            \item Мониторить норму градиента для диагностики проблем
            \item Для плохо обусловленных задач применять предобуславливание
        \end{enumerate}
    \end{enumerate}

    \item \textbf{Выводы по разделу:}
    \begin{enumerate}
        \item \textbf{Критическая важность выбора правильного шага $\alpha$}:
        \begin{itemize}
            \item Оптимальный $\alpha$ обеспечивает сходимость за 16 итераций
            \item Неоптимальный $\alpha$ может увеличить время сходимости в 8 раз
            \item Превышение $\alpha_{\max}$ приводит к расходимости алгоритма
        \end{itemize}

        \item \textbf{Прямая зависимость скорости сходимости от $\kappa$}:
        \begin{itemize}
            \item При увеличении $\kappa$ с 2.6 до 10 скорость уменьшается в 3.6 раза
            \item Чем хуже обусловленность, тем больше зигзагов в траектории
            \item Для задач с большим $\kappa$ необходимы специальные методы
        \end{itemize}

        \item \textbf{Наглядная геометрическая интерпретация метода}:
        \begin{itemize}
            \item Линии уровня визуализируют «ландшафт» функции
            \item Траектории показывают путь градиентного спуска
            \item Собственные векторы объясняют особенности движения
        \end{itemize}

        \item \textbf{Ограничения базового градиентного спуска и направления для улучшения}:
        \begin{enumerate}
            \item \textbf{Ограничения}:
            \begin{itemize}
                \item Чувствительность к выбору шага
                \item Медленная сходимость при плохой обусловленности
                \item Требование вычисления градиента на каждой итерации
            \end{itemize}

            \item \textbf{Направления улучшения}:
            \begin{itemize}
                \item Методы с адаптивным шагом (Adam, RMSprop)
                \item Градиентный спуск с моментом (Momentum)
                \item Методы второго порядка (Ньютона, квазиньютоновские)
                \item Предобуславливание матрицы
            \end{itemize}
        \end{enumerate}
    \end{enumerate}
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{gradient_descent_convergence}
\caption{Сходимость функции $f(x_k)$ при разных значениях $\alpha$}
\label{fig:convergence_alpha}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{gradient_descent_contours_separate}
\caption{Траектории градиентного спуска на линиях уровня при разных $\alpha$}
\label{fig:trajectories_alpha}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{condition_number_convergence}
\caption{Влияние числа обусловленности $\kappa$ на сходимость}
\label{fig:kappa_convergence}
\end{figure}

\end{document}
